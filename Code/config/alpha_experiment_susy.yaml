
logging.path: "./lightning_logs/new/${dataset.name}"
logging.name: "model=${model._target_}-backbone=${model.nn.backbone._target_}-lr=${model.optimizer.lr}-width=${model.nn.backbone.d_model}-loss=${model.loss._target_}-seed=${random.seed}-missingness=${dataset.aux_feat_prob}"

dataset._target_: dataset.SusyDataModule
dataset.name: "SUSY"
dataset.num_workers: 1
dataset.persistent_workers: True
dataset.batch_size: 1
dataset.aux_feat_prob: !!python/tuple [0.2,0.5,0.9,0.95]

random.seed: !!python/tuple [0,1,2,3,4]

trainer.max_epochs: 1
trainer.log_every_n_steps: 1
trainer.devices: 1
trainer.accelerator: 'cpu'
trainer.fast_dev_run: False

model._target_: model.AlphaExperiment
model.loss._target_: torch.nn.CrossEntropyLoss

model.nn.backbone._target_: modules.Fast_AuxDrop_ODL
model.nn.backbone.features_size: 2
model.nn.backbone.n_aux_feat: 6
model.nn.backbone.max_num_hidden_layers: 11
model.nn.backbone.qtd_neuron_per_hidden_layer: 50
model.nn.backbone.n_classes: 2
model.nn.backbone.aux_layer: 3
model.nn.backbone.n_neuron_aux_layer: 100
model.nn.backbone.dropout: 0.3
model.nn.backbone.d_model: 50

model.optimizer._target_: torch.optim.SGD
model.optimizer.lr: 0.05
model.scheduler._target_: torch.optim.lr_scheduler.LinearLR
model.scheduler.warmup_updates: 0
model.scheduler.warmup_end_lr: "${model.optimizer.lr}"

# taskset -c "0-19" python run_hydra.py --config=config/alpha_experiment_susy.yaml