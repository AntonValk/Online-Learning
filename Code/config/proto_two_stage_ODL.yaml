logging.path: "./lightning_logs/new/${dataset.name}"
logging.name: "model=${model._target_}-backbone=${model.nn.backbone._target_}-lr=${model.optimizer.lr}-width=${model.nn.backbone.d_model}-loss=${model.loss._target_}-seed=${random.seed}"

dataset._target_: dataset.GermanDataModule
dataset.name: "german"
dataset.num_workers: 1
dataset.persistent_workers: True
dataset.batch_size: 1
# dataset.eval_batch_size: 128

# random.seed: !!python/tuple [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]
# random.seed: !!python/tuple [0,1,2,3,4]
random.seed: 0

trainer.max_epochs: 1
# trainer.check_val_every_n_epoch: 1
trainer.log_every_n_steps: 1
trainer.devices: 1
trainer.accelerator: 'cpu'
trainer.fast_dev_run: False

model._target_: model.OnlineLearner
model.loss._target_: torch.nn.CrossEntropyLoss

model.nn.backbone._target_: modules.ODLSetSingleStageResidualNet 
model.nn.backbone.d_model: 100
model.nn.backbone.dropout: 0.0
model.nn.backbone.size_in: 1
model.nn.backbone.num_blocks_enc: 2
model.nn.backbone.num_blocks_stage: 2
model.nn.backbone.layer_width_enc: "${model.nn.backbone.d_model}"
model.nn.backbone.layer_width_stage: "${model.nn.backbone.d_model}"
model.nn.backbone.size_out: 2
model.nn.backbone.embedding_dim: 3
model.nn.backbone.num_layers_enc: 2 
model.nn.backbone.num_layers_stage: 2 

model.optimizer._target_: torch.optim.Adam
model.optimizer.lr: 0.001
model.scheduler._target_: torch.optim.lr_scheduler.LinearLR
model.scheduler.warmup_updates: 150
model.scheduler.warmup_end_lr: "${model.optimizer.lr}"

# python run.py --config=config/proto_two_stage_ODL.yaml
# taskset -c "0-4" python run_hydra.py --config=config/proto_two_stage_ODL.yaml
